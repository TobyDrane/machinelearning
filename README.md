# CAP6610 - Machine Learning Course Project
Written by **Jennifer Cheung**, **Joshua Kirstein**, **Abhishek Kumar**, **Abhishek Mohanty**, and **Abhinav Rathi**.


# Installation

To run the code samples in this project, python and some extra libraries are required. To install the libraries run the following commands:

```
sudo pip install pandas
sudo pip install keras
sudo pip install cvxopt
sudo easy_install --upgrade six
```

# Usage

Simply call

```
python main.py
```

or

```
python kernel_svm_test.py
```

**NOTE:** A sequence of plots will be shown. In order to progress through this sequence, you must close the plot that's currently being displayed.

# Multi-Class Kernel SVM
A multi-class kernel support-vector machine was implemented. We build a multi-class kernel SVM as $K$ two-class kernel SVMs, using the one-against-all schema to classify an incoming pattern $x$ (that is, choose the class $k$ who's two-class kernel prediction gives the highest value). A two-class kernel SVM trained with $N$ patterns is constructed by solving the following convex quadratic programming optimization task (using CVXOPT):

\begin{equation*}
\begin{aligned}
& \underset{\{ \lambda_n \}}{\text{maximize}}
& & \sum_{i=1}^{N} \lambda_i - \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i \lambda_j y_i y_j \kappa(x_i, x_j) \\
& \text{subject to}
& & 0 \leq \lambda_i \leq C, \; i = 1, \ldots, N. \\
&&& \sum_{i=1}^{N} \lambda_i y_i = 0
\end{aligned}
\end{equation*}

where $C$ is a constant that helps with outlier rejection and $\kappa(x, y)$ is the kernel function. Once we have $\{ \lambda_n \}$ we compute $\theta_0$ as the average of the following values for each support vector $i$ :

$$ \theta_{0, i} = \frac{1}{y_i} - \sum_{j=1}^{N} \lambda_j y_j \kappa(x_j, x_i)$$

Finally, to classify an incoming pattern $x$ we simply compute

$$y(x) = \sum_{i=1}^{N} \lambda_i y_i \kappa(x_i, x) + \theta_0$$

## Limitations

There are several limitations to this implementation. First of all, the kernel implementation of an SVM is inherently slower due to the kernel computations. Specifically in our implementation, we must compute the Gram matrix for the kernel over the samples --- a computation which is quadratic in the number of patterns. Classifying patterns is similarly deficient; looking at the formula for prediction above, we must compute $N$ kernel values. Varying the kernel changes the performance of the algorithm. Our choice of using python (an interpreted language) also slows down these computations greatly.

Accuracy does not seem to be hindered from this implementation. We use an industrial strength convex optimizer (CVXOPT), which gives much better results than the suggested majorized algorithm. Compared to *libsvm*, our implementation of a multi-class kernel SVM is more accurate (albeit *significantly* slower). Using a Gaussian kernel provided the most accurate classification whilst being the slowest. Using a Linear kernel provided mildly accurate classification whilst being the fastest.

## Testing

We ran the multi-class kernel SVM implementation on multiple data sets. On the handwritten digit data set, our implementation takes a very long time to run (for train percentage less than 0.7 it runs in a moderate amount of time). To see the results from the other data sets, run the code.

To show that our implementation was indeed kernalized, we ran a visual experiment using two-class Gaussian data. The blue and green points are the data colored by their classes. The points in red describe points that are very close to the splitting surface generated by the two-class kernel SVM (using a Gaussian kernel with $\sigma = 20$), thus effectively plotting the splitting surface.

![Kernel visualization #1](http://i.imgur.com/u4iKU5U.png)

![Kernel visualization #2](http://i.imgur.com/Zz1eCUm.png)

# Algorithms Demonstrated

## Neural Network

## Random Forests

## Support Vector Machine

### Introduction

Support Vector Machine, or henceforth SVM, is a supervised learning algorithm that analyzes data for classification. A SVM constructs a hyperplane or a set of hyperplanes in the D-dimesnional space for this classification. 

### Implementation

For Implementation purposes here we have used scikit-learn's svm module in python. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme.The various parameters available in the module were:
i) C values:  Here we have tested a multitude of C values from 0.01 to 1000. We plot below the accuracy we get according to these C values.
ii) Kernel: Trying out multiple options, ‘rbf’ was chosen as it gives the best accuracy on test data. (‘rbf’ here stands for radial base function)
iii) Others parameters are set to default.

[Accuracy vs C_values](http://i.imgur.com/d3COuTZ.png)

### Testing

Data from all our datasets under consideration in divided into training and testing, ranging from 10% training / 90% testing to 50% training / 50% testing. C Value is chosen to giving the best accuracy score. We fit the SVM model on the training data and then try it on test data. The performance metric used is normalized accuracy_score. In this metric the set of labels produced must exactly match the corresponding set of label in its input. The associated plot is displayed.

[Accuracy on Test State](http://i.imgur.com/EmjemQ8.png)


## K Nearest Neighbors

The number of neighbors is cross validated to obtain the accuracies of using different numbers of neighbors.  From this, the optimal number of neighbors is chosen, k.  The incoming patterns are then classified by the majority of the class labels of the closest k feature vectors from the training set.

## KMeans

The number of clusters is crossvalidated to obtain the accuracies of varying amounts of clusters, from one to the number of classes.  From this, the optimal number of clusters is chosen, k, and the centroids, $\mu_k$ of these clusters are saved.  More explicitly, $\mu_0$ is obtained by initializing the clusters, in this case, using sklearn's kmeans++; $\mu_{k+1}$ is obtained by iterateratively taking the means of the feature vectors in the updated cluster obtained from $\mu_k$.  This continues until either $\|\mu_{k+1} - \mu_k\| < 0.0001$ or the number of iterations reaches 300, whichever comes first.  

The class labels of the incoming patterns are determined by the closest centroid.  That is, the class label of the closest centroid to the incoming pattern is assigned as that feature vector's class.

# Testing Methodology

